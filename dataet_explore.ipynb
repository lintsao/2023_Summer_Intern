{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Copyright (C) 2019 NVIDIA Corporation. All rights reserved.\n",
    "# NVIDIA Source Code License (1-Way Commercial)\n",
    "# Code written by Seonwook Park, Shalini De Mello, Yufeng Zheng.\n",
    "# --------------------------------------------------------\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import logging\n",
    "import losses\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import HDFDataset\n",
    "from utils import save_images, worker_init_fn, send_data_dict_to_gpu, recover_images, def_test_list, RunningStatistics,\\\n",
    "    adjust_learning_rate, script_init_common, get_example_images, save_model, load_model\n",
    "from core import DefaultConfig\n",
    "from models import STED\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "config = DefaultConfig()\n",
    "script_init_common()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:39:53,929 Written output/ST-ED/save_2/configs/combined.json\n",
      "2023-08-23 10:39:53,930 Written output/ST-ED/save_2/configs/config_default.py\n",
      "2023-08-23 10:44:16,271 Written source folder to output/ST-ED/save_2/src\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not config.skip_training:\n",
    "    if config.semi_supervised: # Use semi-supervised.\n",
    "        assert config.num_labeled_samples != 0\n",
    "    if not os.path.exists(config.save_path):\n",
    "        os.makedirs(config.save_path)\n",
    "    # save configurations\n",
    "    config.write_file_contents(config.save_path)\n",
    "\n",
    "# Create the train and test datasets.\n",
    "all_data = OrderedDict()\n",
    "\n",
    "# Read GazeCapture train/val/test split\n",
    "with open('./gazecapture_split.json', 'r') as f:\n",
    "    all_gc_prefixes = json.load(f)\n",
    "\n",
    "# [gc/val] full set size:              63518\n",
    "# [gc/val] current set size:           1 270\n",
    "# [gc/test] full set size:            191842\n",
    "# [gc/test] current set size:           3836\n",
    "# [mpi] full set size:                 34790\n",
    "# [mpi] current set size:                695\n",
    "# [gc/train] full set size:          1379083\n",
    "# [gc/train] current set size:       1379083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 10:44:24,872 \n",
      "2023-08-23 10:44:24,873   [gc/val] full set size:             63518\n",
      "2023-08-23 10:44:24,873   [gc/val] current set size:           1270\n",
      "2023-08-23 10:44:24,873 \n",
      "2023-08-23 10:44:24,874  [gc/test] full set size:            191842\n",
      "2023-08-23 10:44:24,875  [gc/test] current set size:           3836\n",
      "2023-08-23 10:44:24,875 \n",
      "2023-08-23 10:44:24,876      [mpi] full set size:             34790\n",
      "2023-08-23 10:44:24,876      [mpi] current set size:            695\n",
      "2023-08-23 10:44:24,877 \n",
      "2023-08-23 10:44:24,877 [gc/train] full set size:           1379083\n",
      "2023-08-23 10:44:24,878 [gc/train] current set size:        1379083\n",
      "2023-08-23 10:44:24,879 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part is to create the dataset.\n",
    "\"\"\"\n",
    "\n",
    "if not config.skip_training:\n",
    "    # Define single training dataset\n",
    "    train_prefixes = all_gc_prefixes['train']\n",
    "    train_dataset = HDFDataset(hdf_file_path=config.gazecapture_file,\n",
    "                               prefixes=train_prefixes,\n",
    "                               is_bgr=False,\n",
    "                               get_2nd_sample=True,\n",
    "                               num_labeled_samples=config.num_labeled_samples if config.semi_supervised else None)\n",
    "    \n",
    "    # Define multiple val/test datasets for evaluation during training\n",
    "    for tag, hdf_file, is_bgr, prefixes in [\n",
    "        ('gc/val', config.gazecapture_file, False, all_gc_prefixes['val']),\n",
    "        ('gc/test', config.gazecapture_file, False, all_gc_prefixes['test']),\n",
    "        ('mpi', config.mpiigaze_file, False, None),\n",
    "    ]:\n",
    "        # Create evaluation dataset.\n",
    "        dataset = HDFDataset(hdf_file_path=hdf_file,\n",
    "                             prefixes=prefixes,\n",
    "                             is_bgr=is_bgr,\n",
    "                             get_2nd_sample=True,\n",
    "                             pick_at_least_per_person=2)\n",
    "        if tag == 'gc/test':\n",
    "            # test pair visualization:\n",
    "            test_list = def_test_list()\n",
    "            test_visualize = get_example_images(dataset, test_list)\n",
    "            test_visualize = send_data_dict_to_gpu(test_visualize, device)\n",
    "\n",
    "        subsample = config.test_subsample\n",
    "        # subsample test sets if requested\n",
    "        if subsample < (1.0 - 1e-6):\n",
    "            dataset = Subset(dataset, np.linspace(\n",
    "                start=0, stop=len(dataset),\n",
    "                num=int(subsample * len(dataset)),\n",
    "                endpoint=False,\n",
    "                dtype=np.uint32,\n",
    "            ))\n",
    "\n",
    "        all_data[tag] = {\n",
    "            'dataset': dataset,\n",
    "            'dataloader': DataLoader(dataset,\n",
    "                                     batch_size=config.eval_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=config.num_data_loaders,  # args.num_data_loaders,\n",
    "                                     pin_memory=True,\n",
    "                                     ),\n",
    "        }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=int(config.batch_size),\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True,\n",
    "                                  num_workers=config.num_data_loaders,\n",
    "                                  pin_memory=True,\n",
    "                                  )\n",
    "    all_data['gc/train'] = {'dataset': train_dataset, 'dataloader': train_dataloader}\n",
    "\n",
    "    # Print some stats.\n",
    "    logging.info('')\n",
    "    for tag, val in all_data.items():\n",
    "        tag = '[%s]' % tag\n",
    "        dataset = val['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        num_people = len(original_dataset.prefixes)\n",
    "        num_original_entries = len(original_dataset)\n",
    "        logging.info('%10s full set size:           %7d' % (tag, num_original_entries))\n",
    "        logging.info('%10s current set size:        %7d' % (tag, len(dataset)))\n",
    "        logging.info('')\n",
    "\n",
    "    # Have dataloader re-open HDF to avoid multi-processing related errors.\n",
    "    for tag, data_dict in all_data.items():\n",
    "        dataset = data_dict['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        original_dataset.close_hdf()\n",
    "\n",
    "# train_dataset.__getitem__(0).keys()\n",
    "# dict_keys(['key', 'image_a', 'gaze_a', 'head_a', 'image_b', 'gaze_b', 'head_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-23 18:46:43,517 Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "2023-08-23 18:46:43,623 Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1908, -0.1203])\n",
      "tensor([-0.0982, -0.0082])\n",
      "tensor([-0.5506,  0.2009])\n",
      "tensor([-0.1445, -0.0589])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "test = all_data['gc/val']['dataset'][1]\n",
    "\n",
    "# GazeCapture training\n",
    "print(test['gaze_a'])\n",
    "print(test['head_a'])\n",
    "plt.imshow(test['image_a'].permute(1, 2, 0))\n",
    "plt.savefig('foo_a.png')\n",
    "\n",
    "print(test['gaze_b'])\n",
    "print(test['head_b'])\n",
    "plt.imshow(test['image_b'].permute(1, 2, 0))\n",
    "plt.savefig('foo_b.png')\n",
    "\n",
    "gaze_yaw_list_a = []\n",
    "gaze_pitch_list_a = []\n",
    "head_yaw_list_a = []\n",
    "head_pitch_list_a = []\n",
    "\n",
    "gaze_yaw_list_b = []\n",
    "gaze_pitch_list_b = []\n",
    "head_yaw_list_b = []\n",
    "head_pitch_list_b = []\n",
    "\n",
    "for i in range(len(all_data['gc/val']['dataset'])):\n",
    "    tmp = all_data['gc/val']['dataset'][i]\n",
    "\n",
    "    gaze_yaw_list_a.append(tmp['gaze_a'][0])\n",
    "    gaze_pitch_list_a.append(tmp['gaze_a'][1])\n",
    "    head_yaw_list_a.append(tmp['head_a'][0])\n",
    "    head_pitch_list_a.append(tmp['head_a'][1])\n",
    "\n",
    "    gaze_yaw_list_b.append(tmp['gaze_a'][0])\n",
    "    gaze_pitch_list_b.append(tmp['gaze_a'][0])\n",
    "    head_yaw_list_b.append(tmp['head_b'][1])\n",
    "    head_pitch_list_b.append(tmp['head_b'][1])\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GazeCapture testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPII testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ours",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
