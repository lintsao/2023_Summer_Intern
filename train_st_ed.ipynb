{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Copyright (C) 2019 NVIDIA Corporation. All rights reserved.\n",
    "# NVIDIA Source Code License (1-Way Commercial)\n",
    "# Code written by Seonwook Park, Shalini De Mello, Yufeng Zheng.\n",
    "# --------------------------------------------------------\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import logging\n",
    "import losses\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import HDFDataset\n",
    "from utils import save_images, worker_init_fn, send_data_dict_to_gpu, recover_images, def_test_list, RunningStatistics,\\\n",
    "    adjust_learning_rate, script_init_common, get_example_images, save_model, load_model\n",
    "from core import DefaultConfig\n",
    "from models import STED\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "config = DefaultConfig()\n",
    "script_init_common()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 21:07:31,460 Written .../ST-ED/configs/combined.json\n",
      "2023-08-07 21:07:31,461 Written .../ST-ED/configs/config_default.py\n",
      "2023-08-07 21:11:01,138 Written source folder to .../ST-ED/src\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not config.skip_training:\n",
    "    if config.semi_supervised: # Use semi-supervised.\n",
    "        assert config.num_labeled_samples != 0\n",
    "    if not os.path.exists(config.save_path):\n",
    "        os.makedirs(config.save_path)\n",
    "    # save configurations\n",
    "    config.write_file_contents(config.save_path)\n",
    "\n",
    "# Create the train and test datasets.\n",
    "all_data = OrderedDict()\n",
    "\n",
    "# Read GazeCapture train/val/test split\n",
    "with open('./gazecapture_split.json', 'r') as f:\n",
    "    all_gc_prefixes = json.load(f)\n",
    "\n",
    "# [gc/val] full set size:              63518\n",
    "# [gc/val] current set size:           1 270\n",
    "# [gc/test] full set size:            191842\n",
    "# [gc/test] current set size:           3836\n",
    "# [mpi] full set size:                 34790\n",
    "# [mpi] current set size:                695\n",
    "# [gc/train] full set size:          1379083\n",
    "# [gc/train] current set size:       1379083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 21:11:10,075 \n",
      "2023-08-07 21:11:10,076   [gc/val] full set size:             63518\n",
      "2023-08-07 21:11:10,077   [gc/val] current set size:           1270\n",
      "2023-08-07 21:11:10,077 \n",
      "2023-08-07 21:11:10,077  [gc/test] full set size:            191842\n",
      "2023-08-07 21:11:10,078  [gc/test] current set size:           3836\n",
      "2023-08-07 21:11:10,078 \n",
      "2023-08-07 21:11:10,078      [mpi] full set size:             34790\n",
      "2023-08-07 21:11:10,079      [mpi] current set size:            695\n",
      "2023-08-07 21:11:10,079 \n",
      "2023-08-07 21:11:10,080 [gc/train] full set size:           1379083\n",
      "2023-08-07 21:11:10,080 [gc/train] current set size:        1379083\n",
      "2023-08-07 21:11:10,082 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part is to create the dataset.\n",
    "\"\"\"\n",
    "\n",
    "if not config.skip_training:\n",
    "    # Define single training dataset\n",
    "    train_prefixes = all_gc_prefixes['train']\n",
    "    train_dataset = HDFDataset(hdf_file_path=config.gazecapture_file,\n",
    "                               prefixes=train_prefixes,\n",
    "                               is_bgr=False,\n",
    "                               get_2nd_sample=True,\n",
    "                               num_labeled_samples=config.num_labeled_samples if config.semi_supervised else None)\n",
    "    \n",
    "    # Define multiple val/test datasets for evaluation during training\n",
    "    for tag, hdf_file, is_bgr, prefixes in [\n",
    "        ('gc/val', config.gazecapture_file, False, all_gc_prefixes['val']),\n",
    "        ('gc/test', config.gazecapture_file, False, all_gc_prefixes['test']),\n",
    "        ('mpi', config.mpiigaze_file, False, None),\n",
    "    ]:\n",
    "        # Create evaluation dataset.\n",
    "        dataset = HDFDataset(hdf_file_path=hdf_file,\n",
    "                             prefixes=prefixes,\n",
    "                             is_bgr=is_bgr,\n",
    "                             get_2nd_sample=True,\n",
    "                             pick_at_least_per_person=2)\n",
    "        if tag == 'gc/test':\n",
    "            # test pair visualization:\n",
    "            test_list = def_test_list()\n",
    "            test_visualize = get_example_images(dataset, test_list)\n",
    "            test_visualize = send_data_dict_to_gpu(test_visualize, device)\n",
    "\n",
    "        subsample = config.test_subsample\n",
    "        # subsample test sets if requested\n",
    "        if subsample < (1.0 - 1e-6):\n",
    "            dataset = Subset(dataset, np.linspace(\n",
    "                start=0, stop=len(dataset),\n",
    "                num=int(subsample * len(dataset)),\n",
    "                endpoint=False,\n",
    "                dtype=np.uint32,\n",
    "            ))\n",
    "\n",
    "        all_data[tag] = {\n",
    "            'dataset': dataset,\n",
    "            'dataloader': DataLoader(dataset,\n",
    "                                     batch_size=config.eval_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=config.num_data_loaders,  # args.num_data_loaders,\n",
    "                                     pin_memory=True,\n",
    "                                     ),\n",
    "        }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=int(config.batch_size),\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True,\n",
    "                                  num_workers=config.num_data_loaders,\n",
    "                                  pin_memory=True,\n",
    "                                  )\n",
    "    all_data['gc/train'] = {'dataset': train_dataset, 'dataloader': train_dataloader}\n",
    "\n",
    "    # Print some stats.\n",
    "    logging.info('')\n",
    "    for tag, val in all_data.items():\n",
    "        tag = '[%s]' % tag\n",
    "        dataset = val['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        num_people = len(original_dataset.prefixes)\n",
    "        num_original_entries = len(original_dataset)\n",
    "        logging.info('%10s full set size:           %7d' % (tag, num_original_entries))\n",
    "        logging.info('%10s current set size:        %7d' % (tag, len(dataset)))\n",
    "        logging.info('')\n",
    "\n",
    "    # Have dataloader re-open HDF to avoid multi-processing related errors.\n",
    "    for tag, data_dict in all_data.items():\n",
    "        dataset = data_dict['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        original_dataset.close_hdf()\n",
    "\n",
    "# train_dataset.__getitem__(0).keys()\n",
    "# dict_keys(['key', 'image_a', 'gaze_a', 'head_a', 'image_b', 'gaze_b', 'head_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: pretrained_models/e4e_ffhq_encode.pt\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/ethentsao/Desktop/Ours/ours/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 21:11:14,763 Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish model initialization.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part is to create the network.\n",
    "\"\"\"\n",
    "\n",
    "# Create redirection network\n",
    "network = STED().to(device)\n",
    "# Load weights if available\n",
    "from checkpoints_manager import CheckpointsManager\n",
    "\n",
    "saver = CheckpointsManager(network.GazeHeadNet_eval, config.eval_gazenet_savepath)\n",
    "_ = saver.load_last_checkpoint()\n",
    "del saver\n",
    "\n",
    "saver = CheckpointsManager(network.GazeHeadNet_train, config.gazenet_savepath)\n",
    "_ = saver.load_last_checkpoint()\n",
    "del saver\n",
    "\n",
    "if config.load_step != 0:\n",
    "    model_path = \"/home/ethentsao/Desktop/STED-gaze/Our fully-supervised gaze redirector model.pt\"\n",
    "    # model_path = os.path.join(config.save_path, \"checkpoints\", str(config.load_step) + '.pt')\n",
    "    print(\"Load model from\", model_path)\n",
    "    load_model(network, model_path)\n",
    "    logging.info(\"Loaded checkpoints from step \" + str(config.load_step))\n",
    "\n",
    "# Transfer on the GPU before constructing and optimizer\n",
    "if torch.cuda.device_count() > 1:\n",
    "    logging.info('Using %d GPUs!' % torch.cuda.device_count())\n",
    "    network.encoder = nn.DataParallel(network.encoder)\n",
    "    network.decoder = nn.DataParallel(network.decoder)\n",
    "    network.redirtrans_p = nn.DataParallel(network.redirtrans_p)\n",
    "    network.redirtrans_dp = nn.DataParallel(network.redirtrans_dp)\n",
    "    network.fusion = nn.DataParallel(network.fusion)\n",
    "    network.discriminator = nn.DataParallel(network.discriminator)\n",
    "    network.GazeHeadNet_eval = nn.DataParallel(network.GazeHeadNet_eval)\n",
    "    network.GazeHeadNet_train = nn.DataParallel(network.GazeHeadNet_train)\n",
    "    network.lpips = nn.DataParallel(network.lpips)\n",
    "    network.pretrained_arcface = nn.DataParallel(network.pretrained_arcface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to prepare for the training step.\n",
    "\"\"\"\n",
    "\n",
    "def execute_training_step(current_step):\n",
    "    global train_data_iterator\n",
    "    try:\n",
    "        input = next(train_data_iterator)\n",
    "    except StopIteration:\n",
    "        np.random.seed()  # Ensure randomness\n",
    "        # Some cleanup\n",
    "        train_data_iterator = None\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect() # 显式地触发垃圾回收。当你想要立即释放内存或者优化内存使用时，这个函数很有用。\n",
    "        # Restart!\n",
    "        global train_dataloader\n",
    "        train_data_iterator = iter(train_dataloader)\n",
    "        input = next(train_data_iterator)\n",
    "    input = send_data_dict_to_gpu(input, device)\n",
    "\n",
    "    network.train()\n",
    "    network.encoder.eval()\n",
    "    network.decoder.eval()\n",
    "    network.GazeHeadNet_eval.eval()\n",
    "    network.GazeHeadNet_train.eval()\n",
    "    network.lpips.eval()\n",
    "    network.pretrained_arcface.eval()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    loss_dict, generated = network.optimize(input, current_step)\n",
    "\n",
    "    # save training samples in tensorboard\n",
    "    if config.use_tensorboard and current_step % config.save_freq_images == 0 and current_step != 0:\n",
    "        for image_index in range(5):\n",
    "            tensorboard.add_image('train/input_image',\n",
    "                                  torch.clamp((input['image_a'][image_index] + 1) * (255.0 / 2.0), 0, 255).type(\n",
    "                                      torch.cuda.ByteTensor), current_step)\n",
    "            tensorboard.add_image('train/target_image',\n",
    "                                  torch.clamp((input['image_b'][image_index] + 1) * (255.0 / 2.0), 0, 255).type(\n",
    "                                      torch.cuda.ByteTensor), current_step)\n",
    "            tensorboard.add_image('train/generated_image',\n",
    "                                  torch.clamp((generated[image_index] + 1) * (255.0 / 2.0), 0, 255).type(\n",
    "                                      torch.cuda.ByteTensor), current_step)\n",
    "    # If doing multi-GPU training, just take an average\n",
    "    for key, value in loss_dict.items():\n",
    "        if value.dim() > 0:\n",
    "            value = torch.mean(value)\n",
    "            loss_dict[key] = value\n",
    "    # Store values for logging later\n",
    "    for key, value in loss_dict.items():\n",
    "        loss_dict[key] = value.detach().cpu()\n",
    "    for key, value in loss_dict.items():\n",
    "        running_losses.add(key, value.numpy())\n",
    "\n",
    "\"\"\"\n",
    "This part is to prepare for the testing step.\n",
    "\"\"\"\n",
    "\n",
    "def execute_test(tag, data_dict):\n",
    "    test_losses = RunningStatistics()\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        for input_dict in tqdm(data_dict['dataloader']):\n",
    "            input_dict = send_data_dict_to_gpu(input_dict, device)\n",
    "            output_dict, loss_dict = network(input_dict)\n",
    "            for key, value in loss_dict.items():\n",
    "                test_losses.add(key, value.detach().cpu().numpy())\n",
    "    test_loss_means = test_losses.means()\n",
    "    logging.info('Test Losses at [%7d] for %10s: %s' %\n",
    "                 (current_step, '[' + tag + ']',\n",
    "                  ', '.join(['%s: %.6f' % v for v in test_loss_means.items()])))\n",
    "    if config.use_tensorboard:\n",
    "        for k, v in test_loss_means.items():\n",
    "            tensorboard.add_scalar('test/%s/%s' % (tag, k), v, current_step)\n",
    "\n",
    "\"\"\"\n",
    "This part is to prepare for the visualization step.\n",
    "\"\"\"\n",
    "\n",
    "def execute_visualize(data):\n",
    "    output_dict, losses_dict = network(test_visualize)\n",
    "    keys = data['key'].cpu().numpy()\n",
    "    for i in tqdm(range(len(keys))):\n",
    "        path = os.path.join(config.save_path, 'samples', str(keys[i]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        cv2.imwrite(os.path.join(path, 'redirect_' + str(current_step) + '.png'),\n",
    "                    recover_images(output_dict['image_b_hat'][i]))\n",
    "        cv2.imwrite(os.path.join(path, 'redirect_all_' + str(current_step) + '.png'),\n",
    "                    recover_images(output_dict['image_b_hat_all'][i]))\n",
    "    # walks = network.latent_walk(test_visualize)\n",
    "    # save_images(os.path.join(config.save_path, 'samples'), walks, keys, cycle=True)\n",
    "\n",
    "\n",
    "if config.use_tensorboard and ((not config.skip_training) or config.compute_full_result):\n",
    "    from tensorboardX import SummaryWriter\n",
    "    if not os.path.exists(config.log_path):\n",
    "        os.mkdir(config.log_path)\n",
    "        print(f\"Make {config.log_path}\")\n",
    "\n",
    "    tensorboard = SummaryWriter(logdir=config.log_path)\n",
    "current_step = config.load_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 21:11:14,798 Training\n",
      "  0%|          | 0/206865 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish lr decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 200/206865 [01:12<19:58:41,  2.87it/s]2023-08-07 21:12:28,344 Losses at [    200]: l1: 1.65070, perceptual: 0.85853, redirection_feature_loss: 18.62209, gaze_redirection: 17.95755, head_redirection: 19.62083, id: 0.96279, gaze_a: 93.59990, head_a: 108.67809, embedding_consistency: 0.01612\n",
      "  0%|          | 400/206865 [02:24<20:23:48,  2.81it/s]2023-08-07 21:13:39,816 Losses at [    400]: l1: 1.68195, perceptual: 0.86340, redirection_feature_loss: 17.95756, gaze_redirection: 17.42968, head_redirection: 19.27347, id: 0.95396, gaze_a: 93.63182, head_a: 108.85184, embedding_consistency: 0.01248\n",
      "  0%|          | 600/206865 [03:35<19:48:43,  2.89it/s]2023-08-07 21:14:50,930 Losses at [    600]: l1: 1.68273, perceptual: 0.86430, redirection_feature_loss: 17.63455, gaze_redirection: 17.95383, head_redirection: 18.55249, id: 0.95384, gaze_a: 93.63341, head_a: 108.74879, embedding_consistency: 0.01041\n",
      "  0%|          | 800/206865 [04:46<19:55:09,  2.87it/s]2023-08-07 21:16:02,291 Losses at [    800]: l1: 1.67219, perceptual: 0.86349, redirection_feature_loss: 17.45338, gaze_redirection: 18.41022, head_redirection: 17.96745, id: 0.95098, gaze_a: 93.57381, head_a: 108.52969, embedding_consistency: 0.00916\n",
      "  0%|          | 1000/206865 [05:58<20:08:43,  2.84it/s]2023-08-07 21:17:14,035 Losses at [   1000]: l1: 1.65941, perceptual: 0.86066, redirection_feature_loss: 17.40101, gaze_redirection: 18.63969, head_redirection: 17.31434, id: 0.94498, gaze_a: 93.73554, head_a: 108.54194, embedding_consistency: 0.00806\n",
      "  1%|          | 1200/206865 [07:10<20:39:08,  2.77it/s]2023-08-07 21:18:25,909 Losses at [   1200]: l1: 1.64577, perceptual: 0.85568, redirection_feature_loss: 17.29810, gaze_redirection: 18.23747, head_redirection: 17.37842, id: 0.93646, gaze_a: 93.68584, head_a: 108.57746, embedding_consistency: 0.00726\n",
      "  1%|          | 1400/206865 [08:22<22:23:46,  2.55it/s]2023-08-07 21:19:38,132 Losses at [   1400]: l1: 1.63231, perceptual: 0.85096, redirection_feature_loss: 17.20471, gaze_redirection: 18.06370, head_redirection: 17.60143, id: 0.93368, gaze_a: 93.73006, head_a: 108.58356, embedding_consistency: 0.00658\n",
      "  1%|          | 1600/206865 [09:34<19:37:57,  2.90it/s]2023-08-07 21:20:50,281 Losses at [   1600]: l1: 1.60898, perceptual: 0.84667, redirection_feature_loss: 17.16122, gaze_redirection: 17.95481, head_redirection: 17.53787, id: 0.93809, gaze_a: 93.72781, head_a: 108.55075, embedding_consistency: 0.00604\n",
      "  1%|          | 1800/206865 [10:46<22:07:28,  2.57it/s]2023-08-07 21:22:02,338 Losses at [   1800]: l1: 1.58285, perceptual: 0.84197, redirection_feature_loss: 17.06926, gaze_redirection: 17.67519, head_redirection: 17.27998, id: 0.94182, gaze_a: 93.69717, head_a: 108.51949, embedding_consistency: 0.00559\n",
      "  1%|          | 2000/206865 [11:59<20:03:53,  2.84it/s]2023-08-07 21:23:14,477 Losses at [   2000]: l1: 1.55136, perceptual: 0.83757, redirection_feature_loss: 17.01869, gaze_redirection: 17.60608, head_redirection: 16.97165, id: 0.94661, gaze_a: 93.68179, head_a: 108.44947, embedding_consistency: 0.00518\n",
      "  1%|          | 2200/206865 [13:11<20:03:10,  2.84it/s]2023-08-07 21:24:26,536 Losses at [   2200]: l1: 1.52022, perceptual: 0.83444, redirection_feature_loss: 16.97111, gaze_redirection: 17.51581, head_redirection: 16.91917, id: 0.94670, gaze_a: 93.67088, head_a: 108.41066, embedding_consistency: 0.00483\n",
      "  1%|          | 2400/206865 [14:23<20:23:28,  2.79it/s]2023-08-07 21:25:39,234 Losses at [   2400]: l1: 1.48354, perceptual: 0.83149, redirection_feature_loss: 16.90882, gaze_redirection: 17.39901, head_redirection: 16.89618, id: 0.94685, gaze_a: 93.64877, head_a: 108.43747, embedding_consistency: 0.00452\n",
      "  1%|▏         | 2600/206865 [15:36<19:49:09,  2.86it/s]2023-08-07 21:26:51,745 Losses at [   2600]: l1: 1.44945, perceptual: 0.82943, redirection_feature_loss: 16.84737, gaze_redirection: 17.45833, head_redirection: 16.89577, id: 0.94880, gaze_a: 93.65344, head_a: 108.41189, embedding_consistency: 0.00426\n",
      "  1%|▏         | 2800/206865 [16:48<21:31:01,  2.63it/s]2023-08-07 21:28:03,770 Losses at [   2800]: l1: 1.41973, perceptual: 0.82712, redirection_feature_loss: 16.79210, gaze_redirection: 17.53378, head_redirection: 16.93934, id: 0.95082, gaze_a: 93.64879, head_a: 108.32666, embedding_consistency: 0.00401\n",
      "  1%|▏         | 3000/206865 [18:00<22:03:54,  2.57it/s]2023-08-07 21:29:15,599 Losses at [   3000]: l1: 1.39327, perceptual: 0.82447, redirection_feature_loss: 16.72070, gaze_redirection: 17.61393, head_redirection: 17.05318, id: 0.95333, gaze_a: 93.60545, head_a: 108.38335, embedding_consistency: 0.00380\n",
      "  2%|▏         | 3200/206865 [19:12<21:57:50,  2.58it/s]2023-08-07 21:30:27,708 Losses at [   3200]: l1: 1.37001, perceptual: 0.82203, redirection_feature_loss: 16.67693, gaze_redirection: 17.66712, head_redirection: 17.13350, id: 0.95545, gaze_a: 93.57961, head_a: 108.45528, embedding_consistency: 0.00360\n",
      "  2%|▏         | 3400/206865 [20:23<19:27:18,  2.91it/s]2023-08-07 21:31:39,322 Losses at [   3400]: l1: 1.34981, perceptual: 0.81952, redirection_feature_loss: 16.66241, gaze_redirection: 17.65706, head_redirection: 17.23369, id: 0.95802, gaze_a: 93.60152, head_a: 108.46043, embedding_consistency: 0.00343\n",
      "  2%|▏         | 3600/206865 [21:35<22:05:04,  2.56it/s]2023-08-07 21:32:51,292 Losses at [   3600]: l1: 1.33151, perceptual: 0.81698, redirection_feature_loss: 16.62559, gaze_redirection: 17.64929, head_redirection: 17.21658, id: 0.95957, gaze_a: 93.63647, head_a: 108.51156, embedding_consistency: 0.00327\n",
      "  2%|▏         | 3800/206865 [22:48<21:26:10,  2.63it/s]2023-08-07 21:34:03,805 Losses at [   3800]: l1: 1.31532, perceptual: 0.81525, redirection_feature_loss: 16.59236, gaze_redirection: 17.67595, head_redirection: 17.22753, id: 0.96060, gaze_a: 93.63360, head_a: 108.52615, embedding_consistency: 0.00312\n",
      "  2%|▏         | 4000/206865 [24:01<22:12:36,  2.54it/s]2023-08-07 21:35:16,495 Losses at [   4000]: l1: 1.30071, perceptual: 0.81358, redirection_feature_loss: 16.55828, gaze_redirection: 17.61961, head_redirection: 17.16018, id: 0.96158, gaze_a: 93.61789, head_a: 108.54714, embedding_consistency: 0.00299\n",
      "  2%|▏         | 4200/206865 [25:13<22:04:33,  2.55it/s]2023-08-07 21:36:29,077 Losses at [   4200]: l1: 1.28739, perceptual: 0.81220, redirection_feature_loss: 16.53543, gaze_redirection: 17.60394, head_redirection: 17.17607, id: 0.96285, gaze_a: 93.60401, head_a: 108.55659, embedding_consistency: 0.00286\n",
      "  2%|▏         | 4400/206865 [26:26<20:43:24,  2.71it/s]2023-08-07 21:37:41,945 Losses at [   4400]: l1: 1.27545, perceptual: 0.81090, redirection_feature_loss: 16.51349, gaze_redirection: 17.56261, head_redirection: 17.15781, id: 0.96344, gaze_a: 93.58606, head_a: 108.59412, embedding_consistency: 0.00275\n",
      "  2%|▏         | 4600/206865 [27:39<20:36:35,  2.73it/s]2023-08-07 21:38:54,947 Losses at [   4600]: l1: 1.26452, perceptual: 0.80976, redirection_feature_loss: 16.49218, gaze_redirection: 17.58108, head_redirection: 17.07025, id: 0.96418, gaze_a: 93.58816, head_a: 108.57992, embedding_consistency: 0.00265\n",
      "  2%|▏         | 4800/206865 [28:51<19:39:30,  2.86it/s]2023-08-07 21:40:07,267 Losses at [   4800]: l1: 1.25439, perceptual: 0.80895, redirection_feature_loss: 16.47467, gaze_redirection: 17.62628, head_redirection: 17.01440, id: 0.96477, gaze_a: 93.57819, head_a: 108.61110, embedding_consistency: 0.00255\n",
      "100%|██████████| 635/635 [06:32<00:00,  1.62it/s]67it/s]\n",
      "2023-08-07 21:47:52,467 Test Losses at [   5000] for   [gc/val]: gaze_a: 93.361298, head_a: 108.164223, head_to_gaze: 1.236387, id_inversion: 0.043866, id_target: 0.987762, gaze_to_head: 1.279459, head_redirection: 21.019077, gaze_redirection: 33.824745, lpips: 0.822093, l1: 1.027985, lpips_all: 0.819668, l1_all: 1.027539\n",
      "100%|██████████| 1918/1918 [19:54<00:00,  1.61it/s]\n",
      "2023-08-07 22:07:46,972 Test Losses at [   5000] for  [gc/test]: gaze_a: 93.193634, head_a: 109.023758, head_to_gaze: 1.252848, id_inversion: 0.043488, id_target: 0.984640, gaze_to_head: 1.278670, head_redirection: 21.595139, gaze_redirection: 33.749317, lpips: 0.820404, l1: 1.030147, lpips_all: 0.817676, l1_all: 1.029397\n",
      "100%|██████████| 348/348 [03:38<00:00,  1.60it/s]\n",
      "2023-08-07 22:11:25,062 Test Losses at [   5000] for      [mpi]: gaze_a: 100.203102, head_a: 98.477776, head_to_gaze: 1.220917, id_inversion: 0.044205, id_target: 0.977720, gaze_to_head: 1.230285, head_redirection: 13.502966, gaze_redirection: 40.568661, lpips: 0.820804, l1: 1.021969, lpips_all: 0.817932, l1_all: 1.021325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish test model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 131.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish visualization.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1673801/39490674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mexecute_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# Print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1673801/1765227127.py\u001b[0m in \u001b[0;36mexecute_training_step\u001b[0;34m(current_step)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             tensorboard.add_image('train/input_image',\n\u001b[0;32m---> 36\u001b[0;31m                                   torch.clamp((input['image_a'][image_index] + 1) * (255.0 / 2.0), 0, 255).type(\n\u001b[0m\u001b[1;32m     37\u001b[0m                                       torch.cuda.ByteTensor), current_step)\n\u001b[1;32m     38\u001b[0m             tensorboard.add_image('train/target_image',\n",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This part is to start thr training step.\n",
    "\"\"\"\n",
    "\n",
    "if not config.skip_training:\n",
    "    logging.info('Training')\n",
    "    running_losses = RunningStatistics()\n",
    "    train_data_iterator = iter(train_dataloader)\n",
    "    # main training loop\n",
    "    for current_step in tqdm(range(config.load_step, config.num_training_steps)): \n",
    "        # Save model\n",
    "        if current_step % config.save_interval == 0 and current_step != config.load_step:\n",
    "            save_model(network, current_step)\n",
    "            print(\"Finish save model.\")\n",
    "\n",
    "        # lr decay\n",
    "        if (current_step % config.decay_steps == 0) or current_step == config.load_step:\n",
    "            lr = adjust_learning_rate(network.optimizers, config.decay, int(current_step /config.decay_steps), config.lr)\n",
    "            if config.use_tensorboard:\n",
    "                tensorboard.add_scalar('train/lr', lr, current_step)\n",
    "            print(\"Finish lr decay.\")\n",
    "\n",
    "        # Testing loop: every specified iterations compute the test statistics\n",
    "        if current_step % config.print_freq_test == 0 and current_step != 0:\n",
    "            network.eval()\n",
    "            network.clean_up()\n",
    "            torch.cuda.empty_cache()\n",
    "            for tag, data_dict in list(all_data.items())[:-1]:\n",
    "                execute_test(tag, data_dict)\n",
    "                # This might help with memory leaks\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Finish test model.\")\n",
    "\n",
    "        # Visualization loop\n",
    "        if (current_step != 0 and current_step % config.save_freq_images == 0) or current_step == config.num_training_steps - 1:\n",
    "            network.eval()\n",
    "            torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                # save redirected, style modified samples\n",
    "                execute_visualize(test_visualize)\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Finish visualization.\")\n",
    "\n",
    "        # Training step\n",
    "        execute_training_step(current_step)\n",
    "\n",
    "        # Print training loss\n",
    "        if current_step != 0 and (current_step % config.print_freq_train == 0):\n",
    "            running_loss_means = running_losses.means()\n",
    "            logging.info('Losses at [%7d]: %s' %\n",
    "                         (current_step,\n",
    "                          ', '.join(['%s: %.5f' % v\n",
    "                                     for v in running_loss_means.items()])))\n",
    "            if config.use_tensorboard:\n",
    "                for k, v in running_loss_means.items():\n",
    "                    tensorboard.add_scalar('train/' + k, v, current_step)\n",
    "            running_losses.reset\n",
    "\n",
    "    logging.info('Finished Training')\n",
    "    # Save model parameters\n",
    "    save_model(network, config.num_training_steps) # Save final model.\n",
    "    del all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 21:04:14,429 Computing complete test results for final model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start execute_visualize\n",
      "test_visualize['image_b'].shape torch.Size([5, 3, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 129.22it/s]\n",
      "2023-08-07 21:04:15,905 \n",
      "2023-08-07 21:04:15,905   [gc/val] set size:                  63518\n",
      "2023-08-07 21:04:15,906   [gc/val] num people:                   48\n",
      "2023-08-07 21:04:15,906 \n",
      "2023-08-07 21:04:15,907  [gc/test] set size:                 191842\n",
      "2023-08-07 21:04:15,907  [gc/test] num people:                  139\n",
      "2023-08-07 21:04:15,908 \n",
      "2023-08-07 21:04:15,908      [mpi] set size:                  34790\n",
      "2023-08-07 21:04:15,908      [mpi] num people:                   15\n",
      "2023-08-07 21:04:15,909 \n",
      "  0%|          | 48/31759 [00:47<8:37:02,  1.02it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1660238/3474664840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mexecute_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tensorboard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtensorboard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1660238/348058632.py\u001b[0m in \u001b[0;36mexecute_test\u001b[0;34m(tag, data_dict)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataloader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0minput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_data_dict_to_gpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0moutput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mtest_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/models/st_ed.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mfusion_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransfer_f_random\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, 1, 512]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0moutput_random_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfusion_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, 3, 1024, 1024]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0moutput_random_a_256\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_random_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch, 3, 256, 256]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_device_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m                     raise RuntimeError(\"module must have its parameters and buffers \"\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m   2079\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m         \"\"\"\n\u001b[0;32m-> 2081\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2113\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m             prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n\u001b[0;32m-> 2115\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2047\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2048\u001b[0m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2049\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2050\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2051\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2266\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2267\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2264\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2266\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2267\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This part is to evaluate.\n",
    "\"\"\"\n",
    "    \n",
    "# Compute evaluation results on complete test sets\n",
    "if config.compute_full_result:\n",
    "    logging.info('Computing complete test results for final model...')\n",
    "    all_data = OrderedDict()\n",
    "    for tag, hdf_file, is_bgr, prefixes in [\n",
    "        ('gc/val', config.gazecapture_file, False, all_gc_prefixes['val']),\n",
    "        ('gc/test', config.gazecapture_file, False, all_gc_prefixes['test']),\n",
    "        ('mpi', config.mpiigaze_file, False, None),\n",
    "    ]:\n",
    "        # Define dataset structure based on selected prefixes\n",
    "        dataset = HDFDataset(hdf_file_path=hdf_file,\n",
    "                             prefixes=prefixes,\n",
    "                             is_bgr=is_bgr,\n",
    "                             get_2nd_sample=True,\n",
    "                             pick_at_least_per_person=2)\n",
    "        if tag == 'gc/test':\n",
    "            # test pair visualization:\n",
    "            test_list = def_test_list()\n",
    "            test_visualize = get_example_images(dataset, test_list)\n",
    "            test_visualize = send_data_dict_to_gpu(test_visualize, device)\n",
    "            with torch.no_grad():\n",
    "                # save redirected, style modified samples\n",
    "                execute_visualize(test_visualize)\n",
    "        all_data[tag] = {\n",
    "            'dataset': dataset,\n",
    "            'dataloader': DataLoader(dataset,\n",
    "                                     batch_size=config.eval_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=config.num_data_loaders,\n",
    "                                     pin_memory=True,\n",
    "                                     worker_init_fn=worker_init_fn),\n",
    "        }\n",
    "    logging.info('')\n",
    "\n",
    "    for tag, val in all_data.items():\n",
    "        tag = '[%s]' % tag\n",
    "        dataset = val['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        num_entries = len(original_dataset)\n",
    "        num_people = len(original_dataset.prefixes)\n",
    "        logging.info('%10s set size:                %7d' % (tag, num_entries))\n",
    "        logging.info('%10s num people:              %7d' % (tag, num_people))\n",
    "        logging.info('')\n",
    "\n",
    "    for tag, data_dict in all_data.items():\n",
    "        dataset = data_dict['dataset']\n",
    "        # Have dataloader re-open HDF to avoid multi-processing related errors.\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        original_dataset.close_hdf()\n",
    "\n",
    "    network.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    for tag, data_dict in list(all_data.items()):\n",
    "        execute_test(tag, data_dict)\n",
    "    if config.use_tensorboard:\n",
    "        tensorboard.close()\n",
    "        del tensorboard\n",
    "    # network.clean_up()\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# \"\"\"\n",
    "# This part is to evaluate.\n",
    "# \"\"\"\n",
    "\n",
    "# # Use Redirector to create new training data\n",
    "# if config.store_redirect_dataset:\n",
    "#     train_tag = 'gc/train'\n",
    "#     train_prefixes = all_gc_prefixes['train']\n",
    "#     train_dataset = HDFDataset(hdf_file_path=config.gazecapture_file,\n",
    "#                                prefixes=train_prefixes,\n",
    "#                                num_labeled_samples=config.num_labeled_samples,\n",
    "#                                sample_target_label=True\n",
    "#                                )\n",
    "#     train_dataset.close_hdf()\n",
    "#     train_dataloader = DataLoader(train_dataset,\n",
    "#                                   batch_size=config.eval_batch_size,\n",
    "#                                   shuffle=False,\n",
    "#                                   num_workers=config.num_data_loaders,\n",
    "#                                   pin_memory=True,\n",
    "#                                   )\n",
    "#     current_person_id = None\n",
    "#     current_person_data = {}\n",
    "#     ofpath = os.path.join(config.save_path, 'Redirected_samples.h5')\n",
    "#     ofdir = os.path.dirname(ofpath)\n",
    "#     if not os.path.isdir(ofdir):\n",
    "#         os.makedirs(ofdir)\n",
    "#     import h5py\n",
    "\n",
    "#     h5f = h5py.File(ofpath, 'w')\n",
    "\n",
    "#     def store_person_predictions():\n",
    "#         global current_person_data\n",
    "#         if len(current_person_data) > 0:\n",
    "#             g = h5f.create_group(current_person_id)\n",
    "#             for key, data in current_person_data.items():\n",
    "#                 g.create_dataset(key, data=data, chunks=tuple([1] + list(np.asarray(data).shape[1:])),\n",
    "#                                  compression='lzf', dtype=\n",
    "#                                  np.float32)\n",
    "#         current_person_data = {}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         np.random.seed()\n",
    "#         num_batches = int(np.ceil(len(train_dataset) / config.eval_batch_size))\n",
    "#         for i, input_dict in enumerate(train_dataloader):\n",
    "#             batch_size = input_dict['image_a'].shape[0]\n",
    "#             input_dict = send_data_dict_to_gpu(input_dict, device)\n",
    "#             output_dict = network.redirect(input_dict)\n",
    "#             zipped_data = zip(\n",
    "#                 input_dict['key'],\n",
    "#                 input_dict['image_a'].cpu().numpy().astype(np.float32),\n",
    "#                 input_dict['gaze_a'].cpu().numpy().astype(np.float32),\n",
    "#                 input_dict['head_a'].cpu().numpy().astype(np.float32),\n",
    "#                 output_dict['image_b_hat_r'].cpu().numpy().astype(np.float32),\n",
    "#                 input_dict['gaze_b_r'].cpu().numpy().astype(np.float32),\n",
    "#                 input_dict['head_b_r'].cpu().numpy().astype(np.float32)\n",
    "#             )\n",
    "\n",
    "#             for (person_id, image_a, gaze_a, head_a, image_b_r, gaze_b_r, head_b_r) in zipped_data:\n",
    "#                 # Store predictions if moved on to next person\n",
    "#                 if person_id != current_person_id:\n",
    "#                     store_person_predictions()\n",
    "#                     current_person_id = person_id\n",
    "#                 # Now write it\n",
    "#                 to_write = {\n",
    "#                     'real': True,\n",
    "#                     'gaze': gaze_a,\n",
    "#                     'head': head_a,\n",
    "#                     'image': image_a,\n",
    "#                 }\n",
    "#                 for k, v in to_write.items():\n",
    "#                     if k not in current_person_data:\n",
    "#                         current_person_data[k] = []\n",
    "#                     current_person_data[k].append(v)\n",
    "\n",
    "#                 to_write = {\n",
    "#                     'real': False,\n",
    "#                     'gaze': gaze_b_r,\n",
    "#                     'head': head_b_r,\n",
    "#                     'image': image_b_r,\n",
    "#                 }\n",
    "#                 for k, v in to_write.items():\n",
    "#                     current_person_data[k].append(v)\n",
    "\n",
    "#             logging.info('processed batch [%04d/%04d] with %d entries.' %\n",
    "#                          (i + 1, num_batches, len(next(iter(input_dict.values())))))\n",
    "#         store_person_predictions()\n",
    "#     logging.info('Completed processing')\n",
    "#     logging.info('Done')\n",
    "#     del train_dataset, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to evaluate.\n",
    "\"\"\"\n",
    "\n",
    "# Use Redirector to create new training data\n",
    "if config.store_redirect_dataset:\n",
    "    train_tag = 'gc/train'\n",
    "    train_prefixes = all_gc_prefixes['train']\n",
    "    train_dataset = HDFDataset(hdf_file_path=config.gazecapture_file,\n",
    "                               prefixes=train_prefixes,\n",
    "                               num_labeled_samples=config.num_labeled_samples,\n",
    "                               sample_target_label=True\n",
    "                               )\n",
    "    train_dataset.close_hdf()\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=config.eval_batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=config.num_data_loaders,\n",
    "                                  pin_memory=True,\n",
    "                                  )\n",
    "    current_person_id = None\n",
    "    current_person_data = {}\n",
    "    ofpath = os.path.join(config.save_path, 'Redirected_samples.h5')\n",
    "    ofdir = os.path.dirname(ofpath)\n",
    "    if not os.path.isdir(ofdir):\n",
    "        os.makedirs(ofdir)\n",
    "    import h5py\n",
    "\n",
    "    h5f = h5py.File(ofpath, 'w')\n",
    "\n",
    "    def store_person_predictions():\n",
    "        global current_person_data\n",
    "        if len(current_person_data) > 0:\n",
    "            g = h5f.create_group(current_person_id)\n",
    "            for key, data in current_person_data.items():\n",
    "                g.create_dataset(key, data=data, chunks=tuple([1] + list(np.asarray(data).shape[1:])),\n",
    "                                 compression='lzf', dtype=\n",
    "                                 np.float32)\n",
    "        current_person_data = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        np.random.seed()\n",
    "        num_batches = int(np.ceil(len(train_dataset) / config.eval_batch_size))\n",
    "        for i, input_dict in enumerate(train_dataloader):\n",
    "            batch_size = input_dict['image_a'].shape[0]\n",
    "            input_dict = send_data_dict_to_gpu(input_dict, device)\n",
    "            output_dict = network.redirect(input_dict)\n",
    "            zipped_data = zip(\n",
    "                input_dict['key'],\n",
    "                input_dict['image_a'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['gaze_a'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['head_a'].cpu().numpy().astype(np.float32),\n",
    "                output_dict['image_b_hat_r'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['gaze_b_r'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['head_b_r'].cpu().numpy().astype(np.float32)\n",
    "            )\n",
    "\n",
    "            for (person_id, image_a, gaze_a, head_a, image_b_r, gaze_b_r, head_b_r) in zipped_data:\n",
    "                # Store predictions if moved on to next person\n",
    "                if person_id != current_person_id:\n",
    "                    store_person_predictions()\n",
    "                    current_person_id = person_id\n",
    "                # Now write it\n",
    "                to_write = {\n",
    "                    'real': True,\n",
    "                    'gaze': gaze_a,\n",
    "                    'head': head_a,\n",
    "                    'image': image_a,\n",
    "                }\n",
    "                for k, v in to_write.items():\n",
    "                    if k not in current_person_data:\n",
    "                        current_person_data[k] = []\n",
    "                    current_person_data[k].append(v)\n",
    "\n",
    "                to_write = {\n",
    "                    'real': False,\n",
    "                    'gaze': gaze_b_r,\n",
    "                    'head': head_b_r,\n",
    "                    'image': image_b_r,\n",
    "                }\n",
    "                for k, v in to_write.items():\n",
    "                    current_person_data[k].append(v)\n",
    "\n",
    "            logging.info('processed batch [%04d/%04d] with %d entries.' %\n",
    "                         (i + 1, num_batches, len(next(iter(input_dict.values())))))\n",
    "        store_person_predictions()\n",
    "    logging.info('Completed processing')\n",
    "    logging.info('Done')\n",
    "    del train_dataset, train_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sted",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
