{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_419943/245217835.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;31m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_add_docstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_einsum\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_lowrank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvd_lowrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_lowrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m from .overrides import (\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from .parameter import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mUninitializedParameter\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedParameter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mUninitializedBuffer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mUninitializedBuffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mReplicationPad3d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZeroPad2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstantPad1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstantPad2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstantPad3d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmbeddingBag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mRNNCellBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTMCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGRUCell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpixelshuffle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPixelShuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPixelUnshuffle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mweight_norm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mweight_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_weight_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvert_parameters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparameters_to_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_to_parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspectral_norm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectral_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_spectral_norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfusion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuse_conv_bn_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuse_conv_bn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmemory_format\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_conv2d_weight_memory_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/utils/spectral_norm.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# This is a top level class because Py2 pickle doesn't like inner class nor an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# instancemethod.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSpectralNormLoadStateDictPreHook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;31m# See docstring of SpectralNorm._version on the changes to spectral_norm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/utils/spectral_norm.py\u001b[0m in \u001b[0;36mSpectralNormLoadStateDictPreHook\u001b[0;34m()\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;31m# This is a top level class because Py2 pickle doesn't like inner class nor an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;31m# instancemethod.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSpectralNormLoadStateDictPreHook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m     \u001b[0;31m# See docstring of SpectralNorm._version on the changes to spectral_norm.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Copyright (C) 2019 NVIDIA Corporation. All rights reserved.\n",
    "# NVIDIA Source Code License (1-Way Commercial)\n",
    "# Code written by Seonwook Park, Shalini De Mello, Yufeng Zheng.\n",
    "# --------------------------------------------------------\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import logging\n",
    "import losses\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dataset import HDFDataset\n",
    "from utils import save_images, worker_init_fn, send_data_dict_to_gpu, recover_images, def_test_list, RunningStatistics,\\\n",
    "    adjust_learning_rate, script_init_common, get_example_images, save_model, load_model\n",
    "from core import DefaultConfig\n",
    "from models import STED\n",
    "logging.basicConfig(format='%(asctime)s %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "config = DefaultConfig()\n",
    "script_init_common()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 19:47:54,940 Written output/ST-ED/save_2/configs/combined.json\n",
      "2023-08-28 19:47:54,941 Written output/ST-ED/save_2/configs/config_default.py\n",
      "2023-08-28 19:54:38,108 Written source folder to output/ST-ED/save_2/src\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if not config.skip_training:\n",
    "    if config.semi_supervised: # Use semi-supervised.\n",
    "        assert config.num_labeled_samples != 0\n",
    "    if not os.path.exists(config.save_path):\n",
    "        os.makedirs(config.save_path)\n",
    "    # save configurations\n",
    "    config.write_file_contents(config.save_path)\n",
    "\n",
    "# Create the train and test datasets.\n",
    "all_data = OrderedDict()\n",
    "\n",
    "# Read GazeCapture train/val/test split\n",
    "with open('./gazecapture_split.json', 'r') as f:\n",
    "    all_gc_prefixes = json.load(f)\n",
    "\n",
    "# [gc/val] full set size:              63518\n",
    "# [gc/val] current set size:           1 270\n",
    "# [gc/test] full set size:            191842\n",
    "# [gc/test] current set size:           3836\n",
    "# [mpi] full set size:                 34790\n",
    "# [mpi] current set size:                695\n",
    "# [gc/train] full set size:          1379083\n",
    "# [gc/train] current set size:       1379083"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 19:54:46,398 \n",
      "2023-08-28 19:54:46,398   [gc/val] full set size:             63518\n",
      "2023-08-28 19:54:46,399   [gc/val] current set size:           1270\n",
      "2023-08-28 19:54:46,400 \n",
      "2023-08-28 19:54:46,400  [gc/test] full set size:            191842\n",
      "2023-08-28 19:54:46,400  [gc/test] current set size:           3836\n",
      "2023-08-28 19:54:46,401 \n",
      "2023-08-28 19:54:46,402      [mpi] full set size:             34790\n",
      "2023-08-28 19:54:46,402      [mpi] current set size:            695\n",
      "2023-08-28 19:54:46,403 \n",
      "2023-08-28 19:54:46,403 [gc/train] full set size:           1379083\n",
      "2023-08-28 19:54:46,404 [gc/train] current set size:        1379083\n",
      "2023-08-28 19:54:46,404 \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part is to create the dataset.\n",
    "\"\"\"\n",
    "\n",
    "if not config.skip_training:\n",
    "    # Define single training dataset\n",
    "    train_prefixes = all_gc_prefixes['train']\n",
    "    train_dataset = HDFDataset(hdf_file_path=config.gazecapture_file,\n",
    "                               prefixes=train_prefixes,\n",
    "                               is_bgr=False,\n",
    "                               get_2nd_sample=True,\n",
    "                               num_labeled_samples=config.num_labeled_samples if config.semi_supervised else None)\n",
    "    \n",
    "    # Define multiple val/test datasets for evaluation during training\n",
    "    for tag, hdf_file, is_bgr, prefixes in [\n",
    "        ('gc/val', config.gazecapture_file, False, all_gc_prefixes['val']),\n",
    "        ('gc/test', config.gazecapture_file, False, all_gc_prefixes['test']),\n",
    "        ('mpi', config.mpiigaze_file, False, None),\n",
    "    ]:\n",
    "        # Create evaluation dataset.\n",
    "        dataset = HDFDataset(hdf_file_path=hdf_file,\n",
    "                             prefixes=prefixes,\n",
    "                             is_bgr=is_bgr,\n",
    "                             get_2nd_sample=True,\n",
    "                             pick_at_least_per_person=2)\n",
    "        if tag == 'gc/test':\n",
    "            # test pair visualization:\n",
    "            test_list = def_test_list()\n",
    "            test_visualize = get_example_images(dataset, test_list)\n",
    "            test_visualize = send_data_dict_to_gpu(test_visualize, device)\n",
    "\n",
    "        subsample = config.test_subsample\n",
    "        # subsample test sets if requested\n",
    "        if subsample < (1.0 - 1e-6):\n",
    "            dataset = Subset(dataset, np.linspace(\n",
    "                start=0, stop=len(dataset),\n",
    "                num=int(subsample * len(dataset)),\n",
    "                endpoint=False,\n",
    "                dtype=np.uint32,\n",
    "            ))\n",
    "\n",
    "        all_data[tag] = {\n",
    "            'dataset': dataset,\n",
    "            'dataloader': DataLoader(dataset,\n",
    "                                     batch_size=config.eval_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=config.num_data_loaders,  # args.num_data_loaders,\n",
    "                                     pin_memory=True,\n",
    "                                     ),\n",
    "        }\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=int(config.batch_size),\n",
    "                                  shuffle=True,\n",
    "                                  drop_last=True,\n",
    "                                  num_workers=config.num_data_loaders,\n",
    "                                  pin_memory=True,\n",
    "                                  )\n",
    "    all_data['gc/train'] = {'dataset': train_dataset, 'dataloader': train_dataloader}\n",
    "\n",
    "    # Print some stats.\n",
    "    logging.info('')\n",
    "    for tag, val in all_data.items():\n",
    "        tag = '[%s]' % tag\n",
    "        dataset = val['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        num_people = len(original_dataset.prefixes)\n",
    "        num_original_entries = len(original_dataset)\n",
    "        logging.info('%10s full set size:           %7d' % (tag, num_original_entries))\n",
    "        logging.info('%10s current set size:        %7d' % (tag, len(dataset)))\n",
    "        logging.info('')\n",
    "\n",
    "    # Have dataloader re-open HDF to avoid multi-processing related errors.\n",
    "    for tag, data_dict in all_data.items():\n",
    "        dataset = data_dict['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        original_dataset.close_hdf()\n",
    "\n",
    "# train_dataset.__getitem__(0).keys()\n",
    "# dict_keys(['key', 'image_a', 'gaze_a', 'head_a', 'image_b', 'gaze_b', 'head_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading e4e over the pSp framework from checkpoint: pretrained_models/e4e_ffhq_encode.pt\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /home/ethentsao/Desktop/Ours/ours/lib/python3.8/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 19:54:51,251 Using 2 GPUs!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish model initialization.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part is to create the network.\n",
    "\"\"\"\n",
    "\n",
    "# Create redirection network\n",
    "network = STED().to(device)\n",
    "# Load weights if available\n",
    "from checkpoints_manager import CheckpointsManager\n",
    "\n",
    "saver = CheckpointsManager(network.GazeHeadNet_eval, config.eval_gazenet_savepath)\n",
    "_ = saver.load_last_checkpoint()\n",
    "del saver\n",
    "\n",
    "saver = CheckpointsManager(network.GazeHeadNet_train, config.gazenet_savepath)\n",
    "_ = saver.load_last_checkpoint()\n",
    "del saver\n",
    "\n",
    "if config.load_step != 0:\n",
    "    model_path = \"/home/ethentsao/Desktop/STED-gaze/Our fully-supervised gaze redirector model.pt\"\n",
    "    # model_path = os.path.join(config.save_path, \"checkpoints\", str(config.load_step) + '.pt')\n",
    "    print(\"Load model from\", model_path)\n",
    "    load_model(network, model_path)\n",
    "    logging.info(\"Loaded checkpoints from step \" + str(config.load_step))\n",
    "\n",
    "# Transfer on the GPU before constructing and optimizer\n",
    "if torch.cuda.device_count() > 1:\n",
    "    logging.info('Using %d GPUs!' % torch.cuda.device_count())\n",
    "    network.encoder = nn.DataParallel(network.encoder)\n",
    "    network.decoder = nn.DataParallel(network.decoder)\n",
    "    network.redirtrans_p = nn.DataParallel(network.redirtrans_p)\n",
    "    network.redirtrans_dp = nn.DataParallel(network.redirtrans_dp)\n",
    "    network.fusion = nn.DataParallel(network.fusion)\n",
    "    network.discriminator = nn.DataParallel(network.discriminator)\n",
    "    network.GazeHeadNet_eval = nn.DataParallel(network.GazeHeadNet_eval)\n",
    "    network.GazeHeadNet_train = nn.DataParallel(network.GazeHeadNet_train)\n",
    "    network.lpips = nn.DataParallel(network.lpips)\n",
    "    network.pretrained_arcface = nn.DataParallel(network.pretrained_arcface)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to prepare for the training step.\n",
    "\"\"\"\n",
    "\n",
    "def execute_training_step(current_step):\n",
    "    global train_data_iterator\n",
    "    try:\n",
    "        input = next(train_data_iterator)\n",
    "    except StopIteration:\n",
    "        np.random.seed()  # Ensure randomness\n",
    "        # Some cleanup\n",
    "        train_data_iterator = None\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect() # 显式地触发垃圾回收。当你想要立即释放内存或者优化内存使用时，这个函数很有用。\n",
    "        # Restart!\n",
    "        global train_dataloader\n",
    "        train_data_iterator = iter(train_dataloader)\n",
    "        input = next(train_data_iterator)\n",
    "    input = send_data_dict_to_gpu(input, device)\n",
    "\n",
    "    network.train()\n",
    "    network.encoder.eval()\n",
    "    network.decoder.eval()\n",
    "    network.GazeHeadNet_eval.eval()\n",
    "    network.GazeHeadNet_train.eval()\n",
    "    network.lpips.eval()\n",
    "    network.pretrained_arcface.eval()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    loss_dict, generated = network.optimize(input, current_step)\n",
    "\n",
    "    # save training samples in tensorboard (Comment it to avoid the error)\n",
    "    # if config.use_tensorboard and current_step % config.save_freq_images == 0 and current_step != 0:\n",
    "    #     for image_index in range(len(input['image_a'])):\n",
    "    #         tensorboard.add_image('train/input_image',\n",
    "    #                               torch.clamp((input['image_a'][image_index] + 1) * (255.0 / 2.0), 0, 255).type(\n",
    "    #                                   torch.cuda.ByteTensor), current_step)\n",
    "    #         tensorboard.add_image('train/target_image',\n",
    "    #                               torch.clamp((input['image_b'][image_index] + 1) * (255.0 / 2.0), 0, 255).type(\n",
    "    #                                   torch.cuda.ByteTensor), current_step)\n",
    "    #         tensorboard.add_image('train/generated_image',\n",
    "    #                               torch.clamp((generated[image_index] + 1) * (255.0 / 2.0), 0, 255).type(\n",
    "    #                                   torch.cuda.ByteTensor), current_step)\n",
    "    \n",
    "    # If doing multi-GPU training, just take an average\n",
    "    for key, value in loss_dict.items():\n",
    "        if value.dim() > 0:\n",
    "            value = torch.mean(value)\n",
    "            loss_dict[key] = value\n",
    "    # Store values for logging later\n",
    "    for key, value in loss_dict.items():\n",
    "        loss_dict[key] = value.detach().cpu()\n",
    "    for key, value in loss_dict.items():\n",
    "        running_losses.add(key, value.numpy())\n",
    "\n",
    "\"\"\"\n",
    "This part is to prepare for the testing step.\n",
    "\"\"\"\n",
    "\n",
    "def execute_test(tag, data_dict):\n",
    "    test_losses = RunningStatistics()\n",
    "    with torch.no_grad():\n",
    "        network.eval()\n",
    "        for input_dict in tqdm(data_dict['dataloader']):\n",
    "            input_dict = send_data_dict_to_gpu(input_dict, device)\n",
    "            output_dict, loss_dict = network(input_dict)\n",
    "            for key, value in loss_dict.items():\n",
    "                test_losses.add(key, value.detach().cpu().numpy())\n",
    "    test_loss_means = test_losses.means()\n",
    "    logging.info('Test Losses at [%7d] for %10s: %s' %\n",
    "                 (current_step, '[' + tag + ']',\n",
    "                  ', '.join(['%s: %.6f' % v for v in test_loss_means.items()])))\n",
    "    if config.use_tensorboard:\n",
    "        for k, v in test_loss_means.items():\n",
    "            tensorboard.add_scalar('test/%s/%s' % (tag, k), v, current_step)\n",
    "\n",
    "\"\"\"\n",
    "This part is to prepare for the visualization step.\n",
    "\"\"\"\n",
    "\n",
    "def execute_visualize(data):\n",
    "    output_dict, losses_dict = network(test_visualize)\n",
    "    keys = data['key'].cpu().numpy()\n",
    "    for i in tqdm(range(len(keys))):\n",
    "        path = os.path.join(config.save_path, 'samples', str(keys[i]))\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        cv2.imwrite(os.path.join(path, 'redirect_' + str(current_step) + '.png'),\n",
    "                    recover_images(output_dict['image_b_hat'][i]))\n",
    "        cv2.imwrite(os.path.join(path, 'redirect_all_' + str(current_step) + '.png'),\n",
    "                    recover_images(output_dict['image_b_hat_all'][i]))\n",
    "    # walks = network.latent_walk(test_visualize)\n",
    "    # save_images(os.path.join(config.save_path, 'samples'), walks, keys, cycle=True)\n",
    "\n",
    "\n",
    "if config.use_tensorboard and ((not config.skip_training) or config.compute_full_result):\n",
    "    from tensorboardX import SummaryWriter\n",
    "    if not os.path.exists(config.log_path):\n",
    "        os.mkdir(config.log_path)\n",
    "        print(f\"Make {config.log_path}\")\n",
    "\n",
    "    tensorboard = SummaryWriter(logdir=config.log_path)\n",
    "current_step = config.load_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 19:55:50,025 Training\n",
      "  0%|          | 0/206865 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish lr decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/206865 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_319808/1929440803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mexecute_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Print training loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_319808/86602341.py\u001b[0m in \u001b[0;36mexecute_training_step\u001b[0;34m(current_step)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# save training samples in tensorboard (Comment it to avoid the error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/models/st_ed.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, data, current_step)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;31m# Reconstruction image Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mlosses_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreconstruction_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeff_l1_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/losses.py\u001b[0m in \u001b[0;36mreconstruction_l1_loss\u001b[0;34m(x, x_hat)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreconstruction_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL1Loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36ml1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3261\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3263\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3264\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Ours/ours/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part is to start thr training step.\n",
    "\"\"\"\n",
    "###########################################################################################################\n",
    "\n",
    "if not config.skip_training:\n",
    "    logging.info('Training')\n",
    "    running_losses = RunningStatistics()\n",
    "    train_data_iterator = iter(train_dataloader)\n",
    "    # main training loop\n",
    "    for current_step in tqdm(range(config.load_step, config.num_training_steps)): \n",
    "        # Save model\n",
    "        if current_step % config.save_interval == 0 and current_step != config.load_step:\n",
    "            save_model(network, current_step)\n",
    "            print(\"Finish save model.\")\n",
    "\n",
    "        # lr decay\n",
    "        if (current_step % config.decay_steps == 0) or current_step == config.load_step:\n",
    "            lr = adjust_learning_rate(network.optimizers, config.decay, int(current_step /config.decay_steps), config.lr)\n",
    "            if config.use_tensorboard:\n",
    "                tensorboard.add_scalar('train/lr', lr, current_step)\n",
    "            print(\"Finish lr decay.\")\n",
    "\n",
    "        # Testing loop: every specified iterations compute the test statistics\n",
    "        if current_step % config.print_freq_test == 0 and current_step != 0:\n",
    "            network.eval()\n",
    "            network.clean_up()\n",
    "            torch.cuda.empty_cache()\n",
    "            for tag, data_dict in list(all_data.items())[:-1]:\n",
    "                execute_test(tag, data_dict)\n",
    "                # This might help with memory leaks\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"Finish test model.\")\n",
    "\n",
    "        # # Visualization loop\n",
    "        if (current_step != 0 and current_step % config.save_freq_images == 0) or current_step == config.num_training_steps - 1:\n",
    "            network.eval()\n",
    "            torch.cuda.empty_cache()\n",
    "            with torch.no_grad():\n",
    "                # save redirected, style modified samples\n",
    "                execute_visualize(test_visualize)\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"Finish visualization.\")\n",
    "\n",
    "        # Training step\n",
    "        execute_training_step(current_step)\n",
    "\n",
    "        # Print training loss\n",
    "        if current_step != 0 and (current_step % config.print_freq_train == 0):\n",
    "            running_loss_means = running_losses.means()\n",
    "            logging.info('Losses at [%7d]: %s' %\n",
    "                         (current_step,\n",
    "                          ', '.join(['%s: %.5f' % v\n",
    "                                     for v in running_loss_means.items()])))\n",
    "            if config.use_tensorboard:\n",
    "                for k, v in running_loss_means.items():\n",
    "                    tensorboard.add_scalar('train/' + k, v, current_step)\n",
    "            running_losses.reset\n",
    "\n",
    "    logging.info('Finished Training')\n",
    "    # Save model parameters\n",
    "    save_model(network, config.num_training_steps) # Save final model.\n",
    "    del all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to evaluate.\n",
    "\"\"\"\n",
    "\n",
    "# Compute evaluation results on complete test sets\n",
    "if config.compute_full_result:\n",
    "    logging.info('Computing complete test results for final model...')\n",
    "    all_data = OrderedDict()\n",
    "    for tag, hdf_file, is_bgr, prefixes in [\n",
    "        ('gc/val', config.gazecapture_file, False, all_gc_prefixes['val']),\n",
    "        ('gc/test', config.gazecapture_file, False, all_gc_prefixes['test']),\n",
    "        ('mpi', config.mpiigaze_file, False, None),\n",
    "    ]:\n",
    "        # Define dataset structure based on selected prefixes\n",
    "        dataset = HDFDataset(hdf_file_path=hdf_file,\n",
    "                             prefixes=prefixes,\n",
    "                             is_bgr=is_bgr,\n",
    "                             get_2nd_sample=True,\n",
    "                             pick_at_least_per_person=2)\n",
    "        if tag == 'gc/test':\n",
    "            # test pair visualization:\n",
    "            test_list = def_test_list()\n",
    "            test_visualize = get_example_images(dataset, test_list)\n",
    "            test_visualize = send_data_dict_to_gpu(test_visualize, device)\n",
    "            with torch.no_grad():\n",
    "                # save redirected, style modified samples\n",
    "                execute_visualize(test_visualize)\n",
    "        all_data[tag] = {\n",
    "            'dataset': dataset,\n",
    "            'dataloader': DataLoader(dataset,\n",
    "                                     batch_size=config.eval_batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     num_workers=config.num_data_loaders,\n",
    "                                     pin_memory=True,\n",
    "                                     worker_init_fn=worker_init_fn),\n",
    "        }\n",
    "    logging.info('')\n",
    "\n",
    "    for tag, val in all_data.items():\n",
    "        tag = '[%s]' % tag\n",
    "        dataset = val['dataset']\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        num_entries = len(original_dataset)\n",
    "        num_people = len(original_dataset.prefixes)\n",
    "        logging.info('%10s set size:                %7d' % (tag, num_entries))\n",
    "        logging.info('%10s num people:              %7d' % (tag, num_people))\n",
    "        logging.info('')\n",
    "\n",
    "    for tag, data_dict in all_data.items():\n",
    "        dataset = data_dict['dataset']\n",
    "        # Have dataloader re-open HDF to avoid multi-processing related errors.\n",
    "        original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "        original_dataset.close_hdf()\n",
    "\n",
    "    network.eval()\n",
    "    torch.cuda.empty_cache()\n",
    "    for tag, data_dict in list(all_data.items()):\n",
    "        execute_test(tag, data_dict)\n",
    "    if config.use_tensorboard:\n",
    "        tensorboard.close()\n",
    "        del tensorboard\n",
    "    # network.clean_up()\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This part is to evaluate.\n",
    "\"\"\"\n",
    "\n",
    "# Use Redirector to create new training data\n",
    "if config.store_redirect_dataset:\n",
    "    train_tag = 'gc/train'\n",
    "    train_prefixes = all_gc_prefixes['train']\n",
    "    train_dataset = HDFDataset(hdf_file_path=config.gazecapture_file,\n",
    "                               prefixes=train_prefixes,\n",
    "                               num_labeled_samples=config.num_labeled_samples,\n",
    "                               sample_target_label=True\n",
    "                               )\n",
    "    train_dataset.close_hdf()\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  batch_size=config.eval_batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=config.num_data_loaders,\n",
    "                                  pin_memory=True,\n",
    "                                  )\n",
    "    current_person_id = None\n",
    "    current_person_data = {}\n",
    "    ofpath = os.path.join(config.save_path, 'Redirected_samples.h5')\n",
    "    ofdir = os.path.dirname(ofpath)\n",
    "    if not os.path.isdir(ofdir):\n",
    "        os.makedirs(ofdir)\n",
    "    import h5py\n",
    "\n",
    "    h5f = h5py.File(ofpath, 'w')\n",
    "\n",
    "    def store_person_predictions():\n",
    "        global current_person_data\n",
    "        if len(current_person_data) > 0:\n",
    "            g = h5f.create_group(current_person_id)\n",
    "            for key, data in current_person_data.items():\n",
    "                g.create_dataset(key, data=data, chunks=tuple([1] + list(np.asarray(data).shape[1:])),\n",
    "                                 compression='lzf', dtype=\n",
    "                                 np.float32)\n",
    "        current_person_data = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        np.random.seed()\n",
    "        num_batches = int(np.ceil(len(train_dataset) / config.eval_batch_size))\n",
    "        for i, input_dict in enumerate(train_dataloader):\n",
    "            batch_size = input_dict['image_a'].shape[0]\n",
    "            input_dict = send_data_dict_to_gpu(input_dict, device)\n",
    "            output_dict = network.redirect(input_dict)\n",
    "            zipped_data = zip(\n",
    "                input_dict['key'],\n",
    "                input_dict['image_a'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['gaze_a'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['head_a'].cpu().numpy().astype(np.float32),\n",
    "                output_dict['image_b_hat_r'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['gaze_b_r'].cpu().numpy().astype(np.float32),\n",
    "                input_dict['head_b_r'].cpu().numpy().astype(np.float32)\n",
    "            )\n",
    "\n",
    "            for (person_id, image_a, gaze_a, head_a, image_b_r, gaze_b_r, head_b_r) in zipped_data:\n",
    "                # Store predictions if moved on to next person\n",
    "                if person_id != current_person_id:\n",
    "                    store_person_predictions()\n",
    "                    current_person_id = person_id\n",
    "                # Now write it\n",
    "                to_write = {\n",
    "                    'real': True,\n",
    "                    'gaze': gaze_a,\n",
    "                    'head': head_a,\n",
    "                    'image': image_a,\n",
    "                }\n",
    "                for k, v in to_write.items():\n",
    "                    if k not in current_person_data:\n",
    "                        current_person_data[k] = []\n",
    "                    current_person_data[k].append(v)\n",
    "\n",
    "                to_write = {\n",
    "                    'real': False,\n",
    "                    'gaze': gaze_b_r,\n",
    "                    'head': head_b_r,\n",
    "                    'image': image_b_r,\n",
    "                }\n",
    "                for k, v in to_write.items():\n",
    "                    current_person_data[k].append(v)\n",
    "\n",
    "            logging.info('processed batch [%04d/%04d] with %d entries.' %\n",
    "                         (i + 1, num_batches, len(next(iter(input_dict.values())))))\n",
    "        store_person_predictions()\n",
    "    logging.info('Completed processing')\n",
    "    logging.info('Done')\n",
    "    del train_dataset, train_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sted",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
